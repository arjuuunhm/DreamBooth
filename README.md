# Implementation of Google's Dreambooth: Fine Tuning Text-to-Image Diffusion Models for Subject Driven Generation

3.1 – Introduction
Fine Tuning Text-to-Image Diffusion Models for Subject Driven Generation (Dreambooth) was published at CVPR 2023 by Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. They are all researchers at Google and Nataniel Ruiz is also a Professor at Boston University. The motivation behind this project is that large text-to-image models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts.

The goal of this approach is to take a pre-trained text-to-image model (In the paper it is Imagen and in our implementation it is a Stable Diffusion model) and finetune it so that it learns to bind a certain identifier token [V] with the subject of the 3-5 images. An example of this is if I have a specific photo of my dog. Dreambooth seeks to leverage the pre-trained model’s prior understanding of what a dog is and entangle it with the embedding of my dog’s unique identifier [V]. 
![Screenshot 2024-05-18 at 8 27 19 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/eddb2fa0-83f6-43b8-979d-769bdefa4a5b)

The main contributions that the paper introduces are the following: 
1. Rare-identifier token 
The authors found that using common words messed with the model’s prior understanding of said words, and using gibberish like “xxxyyy55ttt” lead to artifacts in images rendered by the model. They proposed using words that have low frequency in the diffusion model’s language model.
2. Prior-preservation Loss
The fine-tuning process introduces language drift. This leads to the model losing its semantic understanding for the class and overfitting. The authors introduce a prior-preservation loss as a regularization term.
The authors introduce the use of a frozen model for this. The sample images of their class (Ex. a dog) from the frozen model and attempt to recover a noised version of those samples in the fine tuned model. The fine tuned model also attempts to recover noised samples of [V] class. This encourages the fine-tuned model to learn to bind the [V] token with the subject but also not forget its understanding of the class itself.
![Screenshot 2024-05-18 at 8 26 59 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/acc88c23-b4cc-49a9-a6e0-648a298fec39)

3. Proposed the DINO metric which measures subject fidelity of generated images

3.2 – Chosen Result
The results we attempted to replicate were the DreamBooth (Stable Diffusion) DINO results which was introduced by the group. The DINO metric is the average pairwise cosine similarity between the ViT-S/16 DINO embeddings of generated and real images.

Below is a table where you can see the results from the paper. Pay attention to the Stable Diffusion results since that is the one we aimed to replicate. This is because Imagen is not a public model.
![Screenshot 2024-05-18 at 8 27 56 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/073e745d-f48c-403f-b518-e063c16c8531)
It is worth noting that this DINO metric is not perfect because our subject will be in different orientations and scenarios. As you can see in the diagram under real images DINO score is 0.774. We can see DreamBooth with Stable Diffusion has a score of 0.668. 

3.3 – Reimplementation Details
1. Description of Re-implementation Approach
Model Architecture: We use one model of the following stable diffusion pipeline from huggingface: https://huggingface.co/CompVis/stable-diffusion-v1-4.
We decided to use one fine-tuned model only because using two models (the frozen and fine tuned one) as done in the paper ended up being quite memory intensive. (Rather than using a frozen model to sample prior data about a class, we found a dataset of 200 dog images generated by a stable diffusion model). The rest of our Dreambooth Stable Diffusion consists of a tokenizer, text encoder, vae, unet, and noise scheduler. The training process involves fine-tuning the model’s unet in the latent space. For each training step we encode an image (image of our subject [V] dog and prior image of a dog) into the latent space using the model’s VAE. We then gradually apply noise to the images and input them into the unet along with a text embedding for the image’s corresponding prompt (“A photo of a [V] dog” if it's an image of the subject or “A photo of a dog” if its a prior image). We then calculate our “reconstruction loss” by adding the mean-squared error between the denoised latents outputted by the unet and the latents prior to having noise applied to them. This training is done over 3 epochs. In each epoch we perform this denoising on each of the 200 prior images. In these 200 training steps we select one of our five subject images at random.

  Datasets: For images of our subject, in this case the dog, we obtained a dataset provided by the authors and Google. It contains 30 different subjects each with 5 images. So we can use different subjects. For the prior images, we found a dataset online of 200 dogs. These images were all generated using a stable diffusion model. We will discuss how these images being generated from a different pre-trained model could be a drawback in our reflection section.

  Modifications: The main idea of fine-tuning using the reconstruction loss is an idea we got from the paper, but our training pipeline and process were modifications we made since the paper only provided a high-level overview of the training process. 

  code/metric.py: Evaluating the model using the DINO metric
  This involves testing our model using a DINO metric by computing the Cosine similarity between the real and generated images by the model. We do this by first getting the embeddings using DINO pretrained models, and passing it in for the Cosine similarity computation (this is all mentioned in the paper). In the main function, we pass the path to our images in for the real and generated images and compute the metric on these images, and print those values. 

2. Instructions for running the code
Part 1: Being able to train the model
Go to code/train.py
You will need to change the following paths in order to run this training script. Not only do you need to change them so that they align with the subject you want to use in Dreambooth but you will also need to change the paths to match your workspace because right now they are set to Arjun’s (ahm247)


dataset_path: This is the path to the 3-5 images of the subject. In this case it's our [V] dog
classes_path: This is a path to the 200 images of dogs we found online. If you want to use Dreambooth on another dog you can continue using this dataset. However you will need to find a new dataset if you would like to have a new subject. We suggest sampling the pretrained model before you fine tune it to get your prior images.
checkpoint_path: This points to a directory where a checkpoint for your fine-tuned model’s unet will be saved. 

Change the prior_prompt and id_prompt to match your use case
prior_prompt: This is the prompt that the fine tuned text-to-image model will associate with prior images. In this case it is just “A photo of a dog”. This should reflect the class name.
id_prompt: This is the prompt that the fine tuned text-to-image model will associate with your subject’s images. The key difference is that this prompt has the word “mytoken” in it. This “mytoken” corresponds to the [V] identifier that we want to bind to the subject. We will discuss how we came across using “mytoken” as our identifier in the reflection section. You are able to choose a token you’d like. We prefer using “mytoken”. 

On line 156 where we save model weights change the name of the pth to whatever you prefer. Here we named it “main_dog.pth”. Just make sure it has a .pth suffix. 
On line 168 the output image directory is aligned with Arjun’s workspace. Make sure to change that so it reflects your workspace. 
You can run the training script using the command: python train.py

Part 2:
Next go to code/eval.py in order to run inference 

Again you must change the dataset and checkpoint paths. Dataset should be identical to that of train.py. The directory to the checkpoint should be the same as you’ve done in train.py as well. Just make sure you set the .pth name correctly so you are loading in the correct model weights. 
On line 34 there is a list called prompts. You can choose whatever prompts you’d like to generate images of your subject in different domains. It’s important to remember to use the same identifier token as you did in trian.py. This is because the fine tuned model’s goal is to bind the subject to that identifier. 

On line 57 you need to change the output directory path to reflect your workspace. Do the exact same thing you did in train.py
Your output images will be found in the output directory you specified. You can modify the loop at the bottom of eval.py if you don’t want 5 photos for each prompt. 

In regards to running the code/metric.py file, the images are hardcoded with images we tested it with. However, in order to run it, you can replace it with the path of the images you want to test it on. Insert the real images in the “real_images” variable and the outputs you get in the “generated_images”. You can insert multiple images simultaneously and run it that way. We took the max of the cosine similarity matrix to output useful and relevant scores because we are running it on all possible combinations of the real and generated images. 

GPU Requirements:
We trained our model on a V100 GPU. Stable Diffusion Models take up a lot of memory. We were able to reduce a lot of memory by using gradient accumulation and manually offloading memory intensive tensors during training. GPUs like 3090s should be enough to replicate our results however. If you are facing CUDA Out of Memory errors when training our model on such a GPU, then we recommend you use a V100. (Note: The GPU from the paper is an NVIDIA A100).

3.4 – Results
Our reported score for DINO is a 0.315 so we were unable to reproduce the result provided by the paper which was a 0.668. 
Some discrepancies/challenges we faced during re-implementation that could have impacted our results are: 

1. Memory capacity of our GPU. In the paper the authors were using an A100. Since we had access to less powerful compute ability we had to make modifications that could have compromised our model’s ability to learn how to denoise better. Examples of this are us needing to find a dataset of dog images generated from other stable diffusion models. Prior to us using a V100, we were using much slower models that took a few minutes to sample from our diffusion model so sampling 200 images from our model was not an option.
   
2. In the paper they didn’t specify which token to use as an identifier [V]. We found through research that ‘sks’ is a popular choice for [V]. As discussed in the paper, the choice for [V] depends on the text-to-image model’s vocabulary. Because Google is probably using a larger text-to-image model than us, the ‘sks’ token may not be as useful as expected, making it difficult to reproduce their results given the few resources we have. We found that ‘sks’ didn’t perform so well. We saw that our model had a stronger prior semantic understanding of ‘sks’ than we had originally believed since all the photos of dogs with an ‘sks’ token generated dogs in a military setting or with a firearm because an ‘sks’ is a firearm. We found that using a more unique token like ‘mytoken’ yielded better results. We were also unable to get frequency information about the language model for our stable diffusion model, unlike Google, so we weren’t able to find the optimal token out on our own.

3. It can be seen from our results that some of the images contain a lot of noise. This implies that our model is having some difficulties with denoising. The original paper did not describe exactly how to handle applying noise and timesteps for the denoising process, which required us to, so we assumed that we should gradually add noise as the training progresses. This could explain why we have noisy outputs.
   
4. There is a lot of abstraction that comes with the hugging face stable diffusion pipeline. Figuring out exactly how different parts of the pipeline worked was a bit difficult and documentation that answered our confusion were a bit difficult to read/find. Having gaps in knowledge in the pipeline definitely could have contributed to our results not being as we had hoped.
![Screenshot 2024-05-18 at 11 58 39 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/18ba2887-6e75-47d1-b2a3-712d7a22d34d)
Original Dreambooth dog
![Screenshot 2024-05-18 at 11 25 20 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/98e7ecb4-18a6-4caa-9266-a85e24ad7176)
Prompt:"A photo of a mytoken dog on the street"
![Screenshot 2024-05-18 at 11 24 47 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/d111e85d-fa7b-4867-9559-fa9e158586ef)
Prompt:"A photo of a mytoken dog on the street"
![Screenshot 2024-05-18 at 11 24 14 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/22d46fb9-0bfc-4c1d-8f0a-6f84f0e68e04)
Prompt:"A photo of a mytoken dog driving"
![Screenshot 2024-05-18 at 11 23 40 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/01bf0385-ff8e-418b-8f39-8109804b1d58)
Prompt:"A photo of a mytoken dog driving"
![Screenshot 2024-05-18 at 11 23 07 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/7032b5bc-e2e2-4356-9082-20722d7c0943)
Prompt:"A photo of a mytoken dog driving"
![Screenshot 2024-05-18 at 11 27 27 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/95807378-4528-4783-a55b-c6bab17d5794)
Prompt: "A photo of a mytoken dog in Paris"
![Screenshot 2024-05-18 at 11 28 09 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/e8e93e53-7345-4c91-b481-27213e9804a1)
Prompt: "A photo of a mytoken dog in the city"
![Screenshot 2024-05-18 at 11 28 39 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/1d1465bc-e057-46d4-b6a4-89ac7d11b8df)
Prompt: "A photo of a mytoken dog swimming"
![Screenshot 2024-05-18 at 11 29 15 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/b958ffb2-e4da-4310-a94b-a81ebef85add)
Prompt: "A photo of a mytoken dog swimming"

![Screenshot 2024-05-18 at 11 39 44 PM](https://github.com/arjuuunhm/DreamBooth/assets/96384102/1bcbc713-9b42-4bfe-b5af-11cead01b4e4)
Prompt: "A photo of a mytoken dog at the acropolis"

Analysis of Results: 
Above are results from our finetuned model. There are a few images of dogs that appear to be clear and have some resemblence to our original dreambooth dog. Although they are not the same exact dog the dogs that have been rendered have a similar shape/color/size as the dreambooth dog. We can see other photos that also appear to have some resemblence to our dreambooth dog but the outputs are quite noisy. We believe that this is due to our issues with denoising which we discussed above. Lastly there is a photo of pure gaussian noise. This shows that the diffusion model was unable to denoise the image at all. There are a few examples of pure gaussian noise so there is some variability in our results. This can be dependent on prompt complexity as well. We can also see there is some deformaties in some of the images. We think this could have been caused by some instabilities while training, noise issues, or the model itself because when you sample from the pretrained version the outputs aren't entirely realistic looking. 

3.5 – Conclusion and Future Work
We learned a lot from working on this project. Before working on this project we knew a bit about stable diffusion but now we understand a lot more about all the components in the stable diffusion pipeline and what role each component plays. We also understand more about how image processing works in the latent space. On prior coding assignments we didn’t really run into CUDA Out of Memory errors as much before the workloads were suited towards the GPUs. Working on this project we learned a lot more about how GPU programming and memory works. We also got a better understanding of how to use open-source resources like hugging face. We got more practice evaluating our models by using metrics like DINO. We got more hands-on research experience by taking a research paper, like DreamBooth, and implementing it from scratch.  This gave us more insight into the models and architectures to consider when implementing this method, considering the little amount of implementation details we were given. We both were intrigued by lots of the fine-tuning GenAI applications that were happening in industry. We are glad we got the opportunity to experience first-hand work in this area. 

One of our biggest challenges were with the level of noise left in our outputs. We believe that we weren’t able to figure out exactly how to configure the noise-scheduler and unet timesteps when training. We were not able to find how the authors implemented it in their stable diffusion model since they spoke more about the imagen model in the paper and didn’t share their codebase. We were lucky to obtain a better GPU for our training later on as we were working on this project. We did not have this chance for the majority of our project. Some areas to explore are possibly using smaller diffusion models with fewer parameters to reduce computation overhead and memory requirements. Another strategy worth exploring is mixed precision training. We found that when using quantization with 16-bit floating point types we were able to avoid memory issues, but we had a lot of NaNs in our loss calculation so we switched back to 32-bit floating point types. By using mixed-precision we could definitely improve memory performance while having more stability. 
We could extend our model to other forms of generative AI. The main idea of Dream Booth is to take a subject and fine-tune an existing model so that the subject can be inserted into different domains. We feel this objective has the most potential and relevance in the context of generative AI. For example we have heard AI generated music of rappers like Drake. A similar fine-tuning approach on text-to-speech models can be done to bring artists into a new genre of music. This image problem can be transferred to the domain of videos as well. It seems like a similar problem since now we are just adding time as a dimension. It is worth noting though that such models can be dangerous as they could potentially be used to generate fake images of people which could be used for malicious purposes.

3.6 – References
Research paper: https://arxiv.org/pdf/2208.12242 
Dataset inspiration: https://github.com/google/dreambooth/tree/4f887af7970a06fc0cd3adaa1d0b368547d6a1d0/dataset  
Video for theoretical knowledge about the paper: https://www.youtube.com/watch?v=D641lhioXMc 
model card: https://huggingface.co/CompVis/stable-diffusion-v1-4


