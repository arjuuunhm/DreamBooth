{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae024d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: diffusers in /home/ahm247/.local/lib/python3.8/site-packages (0.27.2)\n",
      "Requirement already satisfied: filelock in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (3.0.12)\n",
      "Requirement already satisfied: requests in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (3.10.0)\n",
      "Requirement already satisfied: numpy in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (1.20.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /home/ahm247/.local/lib/python3.8/site-packages (from diffusers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ahm247/.local/lib/python3.8/site-packages (from diffusers) (0.4.2)\n",
      "Requirement already satisfied: Pillow in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (8.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (2021.4.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.61.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.20.2->diffusers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from importlib-metadata->diffusers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (1.26.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: invisible_watermark in /home/ahm247/.local/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /home/ahm247/.local/lib/python3.8/site-packages (4.40.1)\n",
      "Requirement already satisfied: accelerate in /home/ahm247/.local/lib/python3.8/site-packages (0.29.3)\n",
      "Requirement already satisfied: safetensors in /home/ahm247/.local/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied: torch in /home/ahm247/.local/lib/python3.8/site-packages (from invisible_watermark) (2.2.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (1.20.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /home/ahm247/.local/lib/python3.8/site-packages (from invisible_watermark) (4.9.0.80)\n",
      "Requirement already satisfied: Pillow>=6.0.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (8.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (4.61.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ahm247/.local/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ahm247/.local/lib/python3.8/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: psutil in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (2.19.3)\n",
      "Requirement already satisfied: networkx in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (2.5)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (1.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (3.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ahm247/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->invisible_watermark) (12.4.99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch->invisible_watermark) (2.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch->invisible_watermark) (5.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch->invisible_watermark) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers --upgrade\n",
    "!pip install invisible_watermark transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d13950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from IPython.core.debugger import set_trace\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f169af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model to use off huggingface\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "# Path to directory containing images of the subject we want to use dreambooth on\n",
    "dataset_path = '/home/ahm247/dreambooth/dataset/dog6'\n",
    "\n",
    "# Path to our 200 photos of our prior found online. For another class generate the data\n",
    "classes_path = '/home/ahm247/dreambooth/class-images'\n",
    "\n",
    "# Prior and fine-tuning prompts\n",
    "prior_prompt = 'A photo of a dog'\n",
    "id_prompt = 'A photo of a mytoken dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class for prior images\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory \n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d57fdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c696c983b94441b097ad0110bdbcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_pipe = StableDiffusionPipeline.from_pretrained(model_id, \n",
    "                                                torch_dtype=torch.float32,\n",
    "                                                use_safetensors=True,\n",
    "                                                variant=\"fp16\",\n",
    "                                                safety_checker = None,\n",
    "                                                requires_safety_checker = False)\n",
    "finetuned_pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f977ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using approximately 4067.40 MB of GPU memory.\n"
     ]
    }
   ],
   "source": [
    "def ram_used():\n",
    "    memory_bytes = torch.cuda.memory_allocated()\n",
    "    memory_megabytes = memory_bytes / (1024 ** 2)  # Convert bytes to MB\n",
    "    print(f\"Model is using approximately {memory_megabytes:.2f} MB of GPU memory.\")\n",
    "    \n",
    "def memory_usage(tensor):\n",
    "    element_size = tensor.element_size()  # Returns the size in bytes of each element\n",
    "    num_elements = tensor.numel()  # Returns the total number of elements in the tensor\n",
    "    total_memory_bytes = num_elements * element_size\n",
    "    total_memory_mb = total_memory_bytes / (1024 ** 2)  # Convert bytes to megabytes\n",
    "\n",
    "    print(f\"Total memory usage of tensor: {total_memory_mb} MB\")\n",
    "    \n",
    "ram_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6f41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_token = finetuned_pipe.tokenizer(prior_prompt, return_tensors='pt').to(device)\n",
    "id_token = finetuned_pipe.tokenizer(id_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "prior_input_ids = prior_token['input_ids']\n",
    "prior_attention_masks = prior_token['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    prior_encoder_hidden = finetuned_pipe.text_encoder(input_ids=prior_input_ids, attention_mask=prior_attention_masks)\n",
    "    \n",
    "id_input_ids = id_token['input_ids']\n",
    "id_attention_masks = id_token['attention_mask']\n",
    "\n",
    "with torch.no_grad(): \n",
    "    id_encoder_hidden = finetuned_pipe.text_encoder(input_ids=id_input_ids, attention_mask=id_attention_masks)\n",
    "\n",
    "del prior_token\n",
    "del prior_input_ids\n",
    "del prior_attention_masks\n",
    "\n",
    "del id_token\n",
    "del id_input_ids\n",
    "del id_attention_masks\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Setting up the datasets/dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "prior_dataset = CustomImageDataset(directory=classes_path, transform=transform)\n",
    "id_dataset = CustomImageDataset(directory=dataset_path, transform=transform)\n",
    "prior_dataloader = torch.utils.data.DataLoader(prior_dataset, batch_size=1, shuffle=True)\n",
    "id_dataloader = torch.utils.data.DataLoader(id_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34fe2ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using approximately 4075.57 MB of GPU memory.\n"
     ]
    }
   ],
   "source": [
    "ram_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf949491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 10\n",
    "# accumulation_steps = 4\n",
    "# finetuned_pipe.unet.train()\n",
    "# optimizer = optim.AdamW(finetuned_pipe.unet.parameters(), \n",
    "#                         lr=5e-6,\n",
    "#                         betas=(0.9,0.999),\n",
    "#                         weight_decay=1e-2,\n",
    "#                         eps=1e-08)\n",
    "# # optimizer = optim.SGD(finetuned_pipe.unet.parameters(), \n",
    "# #                       lr=5e-6, \n",
    "# #                       weight_decay=1e-2,\n",
    "# #                       momentum=0.9)\n",
    "# mse_loss = nn.MSELoss()\n",
    "# max_timesteps = finetuned_pipe.scheduler.num_train_timesteps\n",
    "\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     total_loss = 0\n",
    "#     num_batches = 0\n",
    "\n",
    "#     for i, prior_images in enumerate(prior_dataloader):\n",
    "        \n",
    "#         prior_images = prior_images.to(torch.float32).to(device)\n",
    "#         id_batch = next(iter(id_dataloader))\n",
    "#         id_images = random.choice(id_batch).unsqueeze(0).to(torch.float32).to(device)\n",
    "\n",
    "#         prior_latent = finetuned_pipe.vae.encode(prior_images).latent_dist.sample()\n",
    "#         prior_latent *= 0.18215\n",
    "#         noisy_prior_latent = prior_latent + torch.randn_like(prior_latent)\n",
    "\n",
    "#         id_latent = finetuned_pipe.vae.encode(id_images).latent_dist.sample()\n",
    "#         id_latent *= 0.18215\n",
    "#         noisy_id_latent = id_latent + torch.randn_like(id_latent)\n",
    "\n",
    "#         denoised_prior_latent = finetuned_pipe.unet(noisy_prior_latent, timestep=max_timesteps, encoder_hidden_states=prior_encoder_hidden.last_hidden_state)      \n",
    "#         denoised_id_latent = finetuned_pipe.unet(noisy_id_latent, timestep=max_timesteps, encoder_hidden_states=id_encoder_hidden.last_hidden_state)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss_pr = mse_loss(denoised_prior_latent.sample, prior_latent)\n",
    "#         loss_id = mse_loss(denoised_id_latent.sample, id_latent)\n",
    "#         loss = loss_id + loss_pr\n",
    "#         loss /= accumulation_steps\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Free up memory again\n",
    "#         del denoised_id_latent\n",
    "#         del noisy_id_latent\n",
    "#         del id_latent\n",
    "#         del denoised_prior_latent\n",
    "#         del noisy_prior_latent\n",
    "#         del prior_latent\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#         if (i + 1) % accumulation_steps == 0: \n",
    "#             optimizer.step() \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         total_loss += loss.item() * accumulation_steps\n",
    "#         num_batches += 1\n",
    "        \n",
    "        \n",
    "#     average_loss = total_loss / num_batches\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c7834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahm247/.local/lib/python3.8/site-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'PNDMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'PNDMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6efd1148504d538b5550ba8a951f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb8627384943b7b512cf473835a62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.0690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531e1d4a56a74da1873f933b8591a0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7028f35baf8342309e885a47242c07f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b94e298547d4622bc48146e3e781f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8f3dd16ee54c468f4081f4cb441eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.4005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42c5edc33224b71be875e0bef7be75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.3640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a85d6da05cf4257a993adc7b0beb9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.3013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18a53ec76c34647add24792268020b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.2894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6420e12037424b8396ca8cc2cf54648e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2574\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "accumulation_steps = 4\n",
    "finetuned_pipe.unet.train()\n",
    "optimizer = optim.AdamW(finetuned_pipe.unet.parameters(), \n",
    "                        lr=5e-6,\n",
    "                        betas=(0.9,0.999),\n",
    "                        weight_decay=1e-2,\n",
    "                        eps=1e-08)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "mse_loss = nn.MSELoss()\n",
    "max_timesteps = finetuned_pipe.scheduler.num_train_timesteps\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    final_image = finetuned_pipe('A photo of a mytoken dog swimming').images[0]\n",
    "    image_output_directory = './my_images'\n",
    "    if not os.path.exists(image_output_directory):\n",
    "        os.makedirs(image_output_directory)\n",
    "    name = 'mytoken_dog_swimming' + str(epoch) + '.jpg'\n",
    "    # Save the image in the directory\n",
    "    image_path = os.path.join(image_output_directory, name)  # You can change the filename extension according to the image format\n",
    "    final_image.save(image_path)\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, prior_images in enumerate(prior_dataloader):\n",
    "        prior_images = prior_images.to(torch.float32).to(device)\n",
    "        id_batch = next(iter(id_dataloader))\n",
    "        id_images = random.choice(id_batch).unsqueeze(0).to(torch.float32).to(device)\n",
    "        \n",
    "        # Add gradual noise\n",
    "        noise_level = torch.rand(1, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prior_latent = finetuned_pipe.vae.encode(prior_images).latent_dist.sample()\n",
    "            prior_latent *= 0.18215\n",
    "            noisy_prior_latent = prior_latent + noise_level * torch.randn_like(prior_latent)\n",
    "\n",
    "            id_latent = finetuned_pipe.vae.encode(id_images).latent_dist.sample()\n",
    "            id_latent *= 0.18215\n",
    "            noisy_id_latent = id_latent + noise_level * torch.randn_like(id_latent)\n",
    "\n",
    "        # Sample a timestep\n",
    "        timestep = torch.randint(0, max_timesteps, (1,), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        denoised_prior_latent = finetuned_pipe.unet(noisy_prior_latent, timestep=timestep, encoder_hidden_states=prior_encoder_hidden.last_hidden_state).sample\n",
    "        denoised_id_latent = finetuned_pipe.unet(noisy_id_latent, timestep=timestep, encoder_hidden_states=id_encoder_hidden.last_hidden_state).sample\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_pr = mse_loss(denoised_prior_latent, prior_latent)\n",
    "        loss_id = mse_loss(denoised_id_latent, id_latent)\n",
    "        loss = (loss_id + loss_pr) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Free up memory\n",
    "        del denoised_id_latent\n",
    "        del noisy_id_latent\n",
    "        del id_latent\n",
    "        del denoised_prior_latent\n",
    "        del noisy_prior_latent\n",
    "        del prior_latent\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ff94627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c686a3884a67458c945dd4f0baaad2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4e04f705804050be3fb8a5ccd7b4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7ddddcfd4f47e2881f201963b79487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607d4bdf48fd4dc8b0e7aad2b05347e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved\n"
     ]
    }
   ],
   "source": [
    "# Save model weights\n",
    "saved_name = 'main_dog.pth'\n",
    "torch.save(finetuned_pipe.unet.state_dict(), os.path.join('.','checkpoints', saved_name))\n",
    "\n",
    "different_prompts = [\n",
    "    'A photo of a mytoken dog swimming',\n",
    "    'A photo of a mytoken dog in a city',\n",
    "    'A photo of a mytoken dog wearing sunglasses',\n",
    "    'A photo of a mytoken dog in Paris'\n",
    "]\n",
    "# Save image from fine-tuned model\n",
    "for prompt in different_prompts: \n",
    "    final_image = finetuned_pipe(prompt).images[0]\n",
    "    image_output_directory = './final_images'\n",
    "    if not os.path.exists(image_output_directory):\n",
    "        os.makedirs(image_output_directory)\n",
    "\n",
    "    # Save the image in the directory\n",
    "    image_path = os.path.join(image_output_directory, prompt + '.jpg')  # You can change the filename extension according to the image format\n",
    "    final_image.save(image_path)\n",
    "\n",
    "print('Images saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7cb4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prior_images in enumerate(prior_dataloader):\n",
    "    prior_images = prior_images.to(torch.float32).to(device)\n",
    "    id_batch = next(iter(id_dataloader))\n",
    "    id_images = random.choice(id_batch).unsqueeze(0).to(torch.float32).to(device)\n",
    "\n",
    "    # Add gradual noise\n",
    "    noise_level = torch.rand(1, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prior_latent = finetuned_pipe.vae.encode(prior_images).latent_dist.sample()\n",
    "        prior_latent *= 0.18215\n",
    "        noisy_prior_latent = prior_latent + noise_level * torch.randn_like(prior_latent)\n",
    "\n",
    "    # Sample a timestep\n",
    "    timestep = torch.randint(0, max_timesteps, (1,), device=device).long()\n",
    "\n",
    "    # Forward pass\n",
    "    denoised_prior_latent = finetuned_pipe.unet(noisy_prior_latent, timestep=timestep, encoder_hidden_states=prior_encoder_hidden.last_hidden_state).sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image = finetuned_pipe.vae.decode(denoised_prior_latent).sample.squeeze(0)\n",
    "        prior_image = finetuned_pipe.vae.decode(prior_latent).sample.squeeze(0)\n",
    "        \n",
    "    reconstructed_image = to_pil(reconstructed_image)\n",
    "    prior_image = to_pil(prior_image)\n",
    "        \n",
    "    image_output_directory = './final_images'\n",
    "    if not os.path.exists(image_output_directory):\n",
    "        os.makedirs(image_output_directory)\n",
    "\n",
    "    # Save the image in the directory\n",
    "    prior_path = os.path.join(image_output_directory, 'prior_image.jpg')  # You can change the filename extension according to the image format\n",
    "    reconstructed_path = os.path.join(image_output_directory, 'reconstructed_image.jpg')\n",
    "    reconstructed_image.save(reconstructed_path)\n",
    "    prior_image.save(prior_path)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b00786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
