{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae024d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: diffusers in /home/ahm247/.local/lib/python3.8/site-packages (0.27.2)\n",
      "Requirement already satisfied: requests in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (2021.4.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ahm247/.local/lib/python3.8/site-packages (from diffusers) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (3.10.0)\n",
      "Requirement already satisfied: Pillow in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (8.2.0)\n",
      "Requirement already satisfied: filelock in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /home/ahm247/.local/lib/python3.8/site-packages (from diffusers) (0.21.4)\n",
      "Requirement already satisfied: numpy in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from diffusers) (1.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (2024.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.61.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.20.2->diffusers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from importlib-metadata->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->diffusers) (1.26.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: invisible_watermark in /home/ahm247/.local/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /home/ahm247/.local/lib/python3.8/site-packages (4.40.1)\n",
      "Requirement already satisfied: accelerate in /home/ahm247/.local/lib/python3.8/site-packages (0.29.3)\n",
      "Requirement already satisfied: safetensors in /home/ahm247/.local/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied: Pillow>=6.0.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (8.2.0)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /home/ahm247/.local/lib/python3.8/site-packages (from invisible_watermark) (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (1.20.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from invisible_watermark) (1.1.1)\n",
      "Requirement already satisfied: torch in /home/ahm247/.local/lib/python3.8/site-packages (from invisible_watermark) (2.2.1)\n",
      "Requirement already satisfied: filelock in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ahm247/.local/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ahm247/.local/lib/python3.8/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (4.61.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahm247/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: psutil in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: jinja2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (3.0.1)\n",
      "Requirement already satisfied: sympy in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (1.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from torch->invisible_watermark) (2.5)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ahm247/.local/lib/python3.8/site-packages (from torch->invisible_watermark) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ahm247/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->invisible_watermark) (12.4.99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch->invisible_watermark) (2.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch->invisible_watermark) (5.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /share/apps/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch->invisible_watermark) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers --upgrade\n",
    "!pip install invisible_watermark transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d13950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from IPython.core.debugger import set_trace\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f169af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model to use off huggingface\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "# Path to directory containing images of the subject we want to use dreambooth on\n",
    "dataset_path = '/home/ahm247/dreambooth/dataset/dog6'\n",
    "\n",
    "# Path to our 200 photos of our prior found online. For another class generate the data\n",
    "classes_path = '/home/ahm247/dreambooth/class-images'\n",
    "\n",
    "# Prior and fine-tuning prompts\n",
    "prior_prompt = 'A dog'\n",
    "id_prompt = 'A sks dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class for prior images\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory \n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1d1adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d57fdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cccaa9c78ea4270a90d36c3b919fb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_pipe = StableDiffusionPipeline.from_pretrained(model_id, \n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                use_safetensors=True,\n",
    "                                                variant=\"fp16\")\n",
    "finetuned_pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6f41e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n"
     ]
    }
   ],
   "source": [
    "prior_token = finetuned_pipe.tokenizer(prior_prompt, return_tensors='pt').to(device)\n",
    "id_token = finetuned_pipe.tokenizer(id_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "prior_input_ids = prior_token['input_ids']\n",
    "prior_attention_masks = prior_token['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    prior_encoder_hidden = finetuned_pipe.text_encoder(input_ids=prior_input_ids, attention_mask=prior_attention_masks)\n",
    "    \n",
    "id_input_ids = id_token['input_ids']\n",
    "id_attention_masks = id_token['attention_mask']\n",
    "\n",
    "with torch.no_grad(): \n",
    "    id_encoder_hidden = finetuned_pipe.text_encoder(input_ids=id_input_ids, attention_mask=id_attention_masks)\n",
    "    \n",
    "print(type(prior_encoder_hidden))\n",
    "print(type(id_encoder_hidden))\n",
    "# Setting up the datasets/dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "prior_dataset = CustomImageDataset(directory=classes_path, transform=transform)\n",
    "id_dataset = CustomImageDataset(directory=dataset_path, transform=transform)\n",
    "prior_dataloader = torch.utils.data.DataLoader(prior_dataset, batch_size=1, shuffle=True)\n",
    "id_dataloader = torch.utils.data.DataLoader(id_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fcb09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prior_encoder_hidden.last_hidden_state))\n",
    "print(type(id_encoder_hidden.last_hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf949491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahm247/.local/lib/python3.8/site-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'PNDMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'PNDMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "Epoch 1, Loss: 2.938199758529663\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 8.12 MiB is free. Including non-PyTorch memory, this process has 22.15 GiB memory in use. Of the allocated memory 21.01 GiB is allocated by PyTorch, and 56.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-be4cc38100f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnoisy_id_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_latent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdenoised_prior_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetuned_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_prior_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior_encoder_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdenoised_id_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetuned_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_id_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_encoder_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                     \u001b[0madditional_residuals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"additional_residuals\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown_intrablock_additional_residuals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m                 sample, res_samples = downsample_block(\n\u001b[0m\u001b[1;32m   1217\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 )[0]\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m                 hidden_states = attn(\n\u001b[1;32m   1280\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/diffusers/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_shortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_scale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 8.12 MiB is free. Including non-PyTorch memory, this process has 22.15 GiB memory in use. Of the allocated memory 21.01 GiB is allocated by PyTorch, and 56.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "accumulation_steps = 4\n",
    "finetuned_pipe.unet.train()\n",
    "optimizer = optim.AdamW(finetuned_pipe.unet.parameters(), \n",
    "                        lr=5e-6,\n",
    "                        betas=(0.9,0.999),\n",
    "                        weight_decay=1e-2,\n",
    "                        eps=1e-08)\n",
    "mse_loss = nn.MSELoss()\n",
    "max_timesteps = finetuned_pipe.scheduler.num_train_timesteps\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    for (prior_images, id_images) in zip(prior_dataloader, id_dataloader):\n",
    "        prior_images = prior_images.to(torch.float16).to(device)\n",
    "        id_images = id_images.to(torch.float16).to(device)\n",
    "\n",
    "        prior_latent = finetuned_pipe.vae.encode(prior_images).latent_dist.sample()\n",
    "        prior_latent *= 0.18215\n",
    "        noisy_prior_latent = prior_latent + torch.randn_like(prior_latent)\n",
    "\n",
    "        id_latent = finetuned_pipe.vae.encode(id_images).latent_dist.sample()\n",
    "        id_latent *= 0.18215\n",
    "        noisy_id_latent = id_latent + torch.randn_like(id_latent)\n",
    "\n",
    "        denoised_prior_latent = finetuned_pipe.unet(noisy_prior_latent, timestep=max_timesteps, encoder_hidden_states=prior_encoder_hidden.last_hidden_state)\n",
    "        denoised_id_latent = finetuned_pipe.unet(noisy_id_latent, timestep=max_timesteps, encoder_hidden_states=id_encoder_hidden.last_hidden_state) \n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_id = mse_loss(denoised_id_latent.sample, id_latent)\n",
    "        loss_pr = mse_loss(denoised_prior_latent.sample, prior_latent)\n",
    "        loss = loss_id + loss_pr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e994eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
